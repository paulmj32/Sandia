plot(1.1^asd)
1.1^asd
library(tidycensus) #api with census
library(sf) #class and functions for vector data
library(tidyverse) #collection of r data tools, including dplyr
library(mapview)
year=2018
### Geometry base - US Counties
options(tigris_use_cache = TRUE) #to cache shapefiles for future sessions
state = "Georgia"
county = c("Cherokee", "Cobb")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
county_map = county_map %>% select(GEOID, NAME)
### Data from ACS-5yr
acs_vars_use = c("B01001B_001", "B01001A_001", "B01003_001")
acs_data = get_acs(geography = "tract", state = state, county = county,
variables=acs_vars_use, year = year, geometry = FALSE)
#long to wide format
acs_data_w = acs_data %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
df = county_map %>%
left_join(acs_data_w, by = "GEOID") %>% #join ACS_5yr data
mutate(QBLACK = B01001B_001 / B01003_001) %>%
mutate(QWHITE = B01001A_001 / B01003_001) %>%
dplyr::select(QWHITE, QBLACK)
white = df %>%
ggplot(aes(fill = QWHITE)) +
geom_sf(color = NA) +
scale_fill_viridis_c(option = "magma")
mapview(df, zcol = c("QWHITE"), legend = TRUE, alpha.regions = 0.32)
mapview(df, zcol = c("QBLACK"), legend = TRUE, alpha.regions = 0.32)
### Subdivision
state = "Georgia"
county = c("Cherokee", "Cobb")
sub_map = get_acs(geography = "county subdivision", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
sub_map = sub_map %>% select(GEOID, NAME)
### Subdivision
sub_map = get_acs(geography = "county subdivision", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
sub_map = sub_map %>% select(GEOID, NAME)
acs_sub = get_acs(geography = "county subdivision", state = state, county = county,
variables=acs_vars_use, year = year, geometry = FALSE)
acs_sub_w = acs_sub %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
df_sub = sub_map %>%
left_join(acs_data_w, by = "GEOID") %>% #join ACS_5yr data
mutate(QBLACK = B01001B_001 / B01003_001) %>%
mutate(QWHITE = B01001A_001 / B01003_001) %>%
dplyr::select(QWHITE, QBLACK)
mapview(df_sub, zcol = c("QBLACK"), legend = TRUE, alpha.regions = 0.32)
df_sub
acs_sub_w
sub_map
df_sub
View(df_sub)
### Subdivision
sub_map = get_acs(geography = "block group", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
sub_map = sub_map %>% select(GEOID, NAME)
acs_sub_w = acs_sub %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
mapview(df_sub, zcol = c("QBLACK"), legend = TRUE, alpha.regions = 0.32)
acs_sub = get_acs(geography = "block group", state = state, county = county,
variables=acs_vars_use, year = year, geometry = FALSE)
df_sub = sub_map %>%
left_join(acs_data_w, by = "GEOID") %>% #join ACS_5yr data
mutate(QBLACK = B01001B_001 / B01003_001) %>%
mutate(QWHITE = B01001A_001 / B01003_001) %>%
dplyr::select(QWHITE, QBLACK)
df_sub
v18 = load_variables(2018, "acs5", cache = TRUE)
View(v18)
mapview(df, zcol = c("QBLACK"), legend = TRUE, alpha.regions = 0.32)
library(tidycensus) #api with census
library(sf) #class and functions for vector data
library(tidyverse) #collection of r data tools, including dplyr
library(mapview)
year=2018
state = "Alabama"
county = c("Madison")
county_map = county_map %>% select(GEOID, NAME)
acs_data = get_acs(geography = "tract", state = state, county = county,
variables=acs_vars_use, year = year, geometry = FALSE)
### Geometry base - US Counties
options(tigris_use_cache = TRUE) #to cache shapefiles for future sessions
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
### Data from ACS-5yr
acs_vars_use = c("B01001B_001", "B01001A_001", "B01003_001")
df = county_map %>%
left_join(acs_data_w, by = "GEOID") %>% #join ACS_5yr data
mutate(QBLACK = B01001B_001 / B01003_001) %>%
mutate(QWHITE = B01001A_001 / B01003_001) %>%
dplyr::select(QWHITE, QBLACK)
#long to wide format
acs_data_w = acs_data %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
#long to wide format
acs_data_w = acs_data %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
acs_data = get_acs(geography = "tract", state = state, county = county,
variables=acs_vars_use, year = year, geometry = FALSE)
#long to wide format
acs_data_w = acs_data %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
df = county_map %>%
left_join(acs_data_w, by = "GEOID") %>% #join ACS_5yr data
mutate(QBLACK = B01001B_001 / B01003_001) %>%
mutate(QWHITE = B01001A_001 / B01003_001) %>%
dplyr::select(QWHITE, QBLACK)
mapview(df, zcol = c("QBLACK"), legend = TRUE, alpha.regions = 0.32)
# LIST OF VARIABLES
v18 = load_variables(2018, "acs5", cache = TRUE)
View(v18)
library(tidyverse)
library(readxl)
library(sf)
library(tidycensus)
library(tidycensus) #api with census
library(sf) #class and functions for vector data
library(tidyverse) #collection of r data tools, including dplyr
library(mapview)
year=2018
### Geometry base - US Counties
options(tigris_use_cache = TRUE) #to cache shapefiles for future sessions
state = "Texas"
county = c("Harris")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
county = c("Harris County")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
View(county_map)
county_map = county_map %>% select(GEOID, NAME)
### Data from ACS-5yr
acs_vars_use = c("B01001B_001", "B01001A_001", "B01003_001")
acs_data = get_acs(geography = "tract", state = state, county = county,
variables=acs_vars_use, year = year, geometry = FALSE)
#long to wide format
acs_data_w = acs_data %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
df = county_map %>%
left_join(acs_data_w, by = "GEOID") %>% #join ACS_5yr data
mutate(QBLACK = B01001B_001 / B01003_001) %>%
mutate(QWHITE = B01001A_001 / B01003_001) %>%
dplyr::select(QWHITE, QBLACK)
white = df %>%
ggplot(aes(fill = QWHITE)) +
geom_sf(color = NA) +
scale_fill_viridis_c(option = "magma")
plot(white)
View(county_map)
write.csv(county_map, "/Users/paulmj/Documents/01_VECTOR/UH_CIVIC/Harris_census_2018.csv", row.names = TRUE)
county_map2 = county_map %>%
st_geometry(NULL)
county_map2 = county_map %>%
st_set_geometry(NULL)
write.csv(county_map2, "/Users/paulmj/Documents/01_VECTOR/UH_CIVIC/Harris_census_2018.csv", row.names = TRUE)
year=2019
### Geometry base - US Counties
options(tigris_use_cache = TRUE) #to cache shapefiles for future sessions
state = "Texas"
county = c("Harris County")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
county_map = county_map %>% select(GEOID, NAME)
### Data from ACS-5yr
acs_vars_use = c("B01001B_001", "B01001A_001", "B01003_001")
acs_data = get_acs(geography = "tract", state = state, county = county,
variables=acs_vars_use, year = year, geometry = FALSE)
#long to wide format
acs_data_w = acs_data %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
df = county_map %>%
left_join(acs_data_w, by = "GEOID") %>% #join ACS_5yr data
mutate(QBLACK = B01001B_001 / B01003_001) %>%
mutate(QWHITE = B01001A_001 / B01003_001) %>%
dplyr::select(QWHITE, QBLACK)
white = df %>%
ggplot(aes(fill = QWHITE)) +
geom_sf(color = NA) +
scale_fill_viridis_c(option = "magma")
black = df %>%
ggplot(aes(fill = QBLACK)) +
geom_sf(color = NA) +
scale_fill_viridis_c(option = "magma")
county_map2 = county_map %>%
st_set_geometry(NULL)
write.csv(county_map2, "/Users/paulmj/Documents/01_VECTOR/UH_CIVIC/Harris_censustract_2019acs.csv", row.names = TRUE)
year=2020
### Geometry base - US Counties
options(tigris_use_cache = TRUE) #to cache shapefiles for future sessions
state = "Texas"
county = c("Harris County")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
?get_acs
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2020, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2019, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2019, geometry = TRUE,
cache_table = TRUE, survey = "acs5")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2019, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2020, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2021, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2019, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = county_map %>% select(GEOID, NAME)
install.packages("stars")
library(tidyverse)
library(tidycensus)
library(sf)
library(terra)
library(stars)
load("~/Documents/01_VECTOR.nosync/Sandia/TX_group.Rda")
## Root zone data
TX_gdb =  "/Users/paulmj/Downloads/gSSURGO_TX/gSSURGO_TX.gdb"
TX_Valu1 = sf::st_read(dsn = TX_gdb, layer = "Valu1")
TX_group_val1 = TX_group %>% left_join(TX_Valu1, by = c("MUKEY" = "mukey"))
TX_group_val1_100 = TX_group_val1 %>%
filter(!is.na(rootznemc)) %>%
slice(1:100) %>%
select(MUKEY, rootznemc)
?st_rasterize
TX_rast100 = st_rasterize(TX_group_val1_100)
TX_rast100 = st_rasterize(TX_group_val1_100["rootznemc"])
plot(TX_rast100)
TX_rast = st_rasterize(TX_group_val1["rootznemc"])
plot(TX_rast)
rr = ggplot() +
geom_tile(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c()
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c()
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
#geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
geom_stars(data = TX_rast, aes(fill = rootznemc)) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc), alpha = 0.8) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc), alpha = 0.9) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1) +
theme_bw()
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1) +
theme_classic()
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1) +
theme_minimal()
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1, na.value = "white")
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1, na.value = "gray")
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1, na.value = "gray") +
theme_light()
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1, na.value = "gray") +
theme_minimal()
rr
st_crs(TX_rast)
st_crs(TX_rast)$units_gdal
TX_rast_1k = st_rasterize(TX_group_val1["rootznemc"], dx = 1000, dy = 1000)
rr = ggplot() +
geom_stars(data = TX_rast_1k, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1, na.value = "gray") +
theme_minimal()
rr
install.packages("sqldf")
version
install.packages("sqldf")
install.packages("hurricaneexposure")
install.packages("caret")
install.packages("tidymodels")
install.packages("tune")
install.packages("workflows")
load("~/Documents/01_VECTOR.nosync/Sandia/Data/Sandia3_ml.RData")
options(java.parameters = "-Xmx7g")
library(bartMachine)
set_bart_machine_num_cores(4)
library(tidyverse)
library(tidymodels)
library(tidycensus)
library(sf)
library(lme4)
library(corrplot)
library(viridis)
#library(sqldf)
# GBM
show_model_info("boost_tree")
?dials
?set_engine
?ranger::importance
?ranger::ranger
?randomForest::randomForest
install.packages("xgboost")
?xgboost::xgb.train
gb_model = boost_tree(mode = "regression", trees = 1000,
min_n = tune(), tree_depth = tune(), learn_rate = tune(), loss_reduction = tune()) %>%
set_engine(engine = "xgboost")
gb_work = workflow() %>%
add_recipe(hours_recipe) %>%
add_model(gb_model)
gb_grid = dials::grid_max_entropy(parameters(min_n, tree_depth, learn_rate, loss_reduction), size = 100)
?min_n
gb_grid = dials::grid_max_entropy(parameters(min_n(), tree_depth(), learn_rate(), loss_reduction()), size = 100)
plot(gb_grid)
?tune_grid
gb_tune = gb_work %>%
tune_grid(resamples = df_cv,
grid = gb_grid,
metrics = metric_set(yardstick::rmse(), yardstick::rsq()))
gb_tune = gb_work %>%
tune_grid(resamples = df_cv,
grid = gb_grid,
metrics = metric_set(yardstick::rmse, yardstick::rsq))
?control_grid
gb_tune = gb_work %>%
tune_grid(resamples = df_cv,
grid = gb_grid,
metrics = metric_set(yardstick::rmse, yardstick::rsq),
control = tune::control_grid(verbose = T))
show_best(gb_tune, metric = "rmse")
save.image("~/Documents/01_VECTOR.nosync/Sandia/Data/Sandia3_ml.RData")
gb_tune_results = gb_tune %>% collect_metrics()
gb_best = gb_tune %>% select_best(metric = "rmse")
gb_fit = gb_work %>%
finalize_workflow(gb_best) %>%
last_fit(df_split)
gb_test = gb_fit %>% collect_metrics() #metrics evaluated on test sample (b/c last_fit() function)
gb_predictions = gb_fit %>% collect_predictions() #predictions for test sample (b/c last_fit() function)
save.image("~/Documents/01_VECTOR.nosync/Sandia/Data/Sandia3_ml.RData")
#### LOAD PACKAGES and SET DIRECTORY
options(java.parameters = "-Xmx7g")
library(bartMachine)
set_bart_machine_num_cores(4)
library(tidyverse)
library(tidymodels)
library(tidycensus)
library(sf)
library(lme4)
library(corrplot)
library(viridis)
library(doParallel)
library(terra)
library(stars)
library(raster) #make sure ncdf4 package is installed
library(lubridate)
library(spdep)
library(xgboost)
library(sqldf)
tidymodels_prefer()
setwd("~/Documents/01_VECTOR.nosync/Sandia")
num_cores = detectCores() - 1
#function to un-register parallel processing in doParallel
unregister_dopar = function() {
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)
}
?`dbConnect,SQLiteConnection-method`
## Import SQL data
# https://stackoverflow.com/questions/9802680/importing-files-with-extension-sqlite-into-r
con = RSQLite::dbConnect(drv = RSQLite::SQLite(), dbname = "./Data/Outages sqlite/AGM_full_CONUS_outages_merra_noaa_summary_Proc31Jan2022.sqlite")
tables = dbListTables(con) #see tables
length_tables = vector("list", length=length(tables))
tables <- tables[tables != "sqlite_sequence"]
length_tables = vector("list", length=length(tables))
#### LOAD PACKAGES and SET DIRECTORY
options(java.parameters = "-Xmx7g")
library(bartMachine)
set_bart_machine_num_cores(4)
library(tidyverse)
library(tidymodels)
library(tidycensus)
library(sf)
library(corrplot)
library(viridis)
library(doParallel)
library(terra)
library(stars)
library(raster) #make sure ncdf4 package is installed
library(lubridate)
library(spdep)
library(xgboost)
#library(sqldf)
library(DBI)
tidymodels_prefer()
setwd("~/Documents/01_VECTOR.nosync/Sandia")
num_cores = detectCores() - 1
#function to un-register parallel processing in doParallel
unregister_dopar = function() {
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)
}
## Import SQL data
# https://stackoverflow.com/questions/9802680/importing-files-with-extension-sqlite-into-r
# https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html
con = RSQLite::dbConnect(drv = RSQLite::SQLite(), dbname = "./Data/Outages sqlite/AGM_full_CONUS_outages_merra_noaa_summary_Proc31Jan2022.sqlite")
tables = dbListTables(con) #see tables
dbDisconnect(con)
tables = dbListTables(con) #see tables
## Import SQL data
# https://stackoverflow.com/questions/9802680/importing-files-with-extension-sqlite-into-r
# https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html
con = RSQLite::dbConnect(drv = RSQLite::SQLite(), dbname = "./Data/Outages sqlite/AGM_full_CONUS_outages_merra_noaa_summary_Proc31Jan2022.sqlite")
tables = dbListTables(con) #see tables
dbGetQuery(mydb, 'SELECT * FROM census LIMIT 5')
## Import SQL data
# https://stackoverflow.com/questions/9802680/importing-files-with-extension-sqlite-into-r
# https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html
mydb = RSQLite::dbConnect(drv = RSQLite::SQLite(), dbname = "./Data/Outages sqlite/AGM_full_CONUS_outages_merra_noaa_summary_Proc31Jan2022.sqlite")
tables = dbListTables(mydb) #see tables
dbGetQuery(mydb, 'SELECT * FROM census LIMIT 5')
dbGetQuery(mydb, 'SELECT * FROM merra LIMIT 5')
dbGetQuery(mydb, 'SELECT * FROM merra LIMIT 10')
dbGetQuery(mydb, 'SELECT TOP 5 * FROM merra')
dbGetQuery(mydb, 'SELECT TOP 2 * FROM merra')
dbGetQuery(mydb, 'SELECT * FROM merra LIMIT 5')
dbGetQuery(mydb, 'SELECT * FROM outages LIMIT 5')
dbGetQuery(mydb, 'SELECT * FROM summary LIMIT 5')
dbGetQuery(mydb, 'SELECT * FROM merra LIMIT 5')
dbGetQuery(mydb, 'SELECT * FROM outages LIMIT 5')
?dbConnect
## Import SQL data
# https://stackoverflow.com/questions/9802680/importing-files-with-extension-sqlite-into-r
# https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html
mydb = RSQLite::dbConnect(drv = RSQLite::SQLite(),
bigint = c("character"),
dbname = "./Data/Outages sqlite/AGM_full_CONUS_outages_merra_noaa_summary_Proc31Jan2022.sqlite")
tables = dbListTables(mydb) #see tables
dbGetQuery(mydb, 'SELECT * FROM outages LIMIT 5')
dbDisconnect(con)
## Import SQL data
# https://stackoverflow.com/questions/9802680/importing-files-with-extension-sqlite-into-r
# https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html
mydb = RSQLite::dbConnect(drv = RSQLite::SQLite(),
bigint = c("character"),
dbname = "./Data/Outages sqlite/AGM_full_CONUS_outages_merra_noaa_summary_Proc31Jan2022.sqlite")
tables = dbListTables(mydb) #see tables
dbGetQuery(mydb, 'SELECT * FROM outages LIMIT 5')
asd = dbGetQuery(mydb, 'SELECT * FROM outages LIMIT 5')
View(asd)
as_datetime(1414814400 )
as_datetime(1414814400)
