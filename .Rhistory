mapview(df, zcol = c("QBLACK"), legend = TRUE, alpha.regions = 0.32)
library(tidycensus) #api with census
library(sf) #class and functions for vector data
library(tidyverse) #collection of r data tools, including dplyr
library(mapview)
year=2018
state = "Alabama"
county = c("Madison")
county_map = county_map %>% select(GEOID, NAME)
acs_data = get_acs(geography = "tract", state = state, county = county,
variables=acs_vars_use, year = year, geometry = FALSE)
### Geometry base - US Counties
options(tigris_use_cache = TRUE) #to cache shapefiles for future sessions
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
### Data from ACS-5yr
acs_vars_use = c("B01001B_001", "B01001A_001", "B01003_001")
df = county_map %>%
left_join(acs_data_w, by = "GEOID") %>% #join ACS_5yr data
mutate(QBLACK = B01001B_001 / B01003_001) %>%
mutate(QWHITE = B01001A_001 / B01003_001) %>%
dplyr::select(QWHITE, QBLACK)
#long to wide format
acs_data_w = acs_data %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
#long to wide format
acs_data_w = acs_data %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
acs_data = get_acs(geography = "tract", state = state, county = county,
variables=acs_vars_use, year = year, geometry = FALSE)
#long to wide format
acs_data_w = acs_data %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
df = county_map %>%
left_join(acs_data_w, by = "GEOID") %>% #join ACS_5yr data
mutate(QBLACK = B01001B_001 / B01003_001) %>%
mutate(QWHITE = B01001A_001 / B01003_001) %>%
dplyr::select(QWHITE, QBLACK)
mapview(df, zcol = c("QBLACK"), legend = TRUE, alpha.regions = 0.32)
# LIST OF VARIABLES
v18 = load_variables(2018, "acs5", cache = TRUE)
View(v18)
library(tidyverse)
library(readxl)
library(sf)
library(tidycensus)
library(tidycensus) #api with census
library(sf) #class and functions for vector data
library(tidyverse) #collection of r data tools, including dplyr
library(mapview)
year=2018
### Geometry base - US Counties
options(tigris_use_cache = TRUE) #to cache shapefiles for future sessions
state = "Texas"
county = c("Harris")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
county = c("Harris County")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
View(county_map)
county_map = county_map %>% select(GEOID, NAME)
### Data from ACS-5yr
acs_vars_use = c("B01001B_001", "B01001A_001", "B01003_001")
acs_data = get_acs(geography = "tract", state = state, county = county,
variables=acs_vars_use, year = year, geometry = FALSE)
#long to wide format
acs_data_w = acs_data %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
df = county_map %>%
left_join(acs_data_w, by = "GEOID") %>% #join ACS_5yr data
mutate(QBLACK = B01001B_001 / B01003_001) %>%
mutate(QWHITE = B01001A_001 / B01003_001) %>%
dplyr::select(QWHITE, QBLACK)
white = df %>%
ggplot(aes(fill = QWHITE)) +
geom_sf(color = NA) +
scale_fill_viridis_c(option = "magma")
plot(white)
View(county_map)
write.csv(county_map, "/Users/paulmj/Documents/01_VECTOR/UH_CIVIC/Harris_census_2018.csv", row.names = TRUE)
county_map2 = county_map %>%
st_geometry(NULL)
county_map2 = county_map %>%
st_set_geometry(NULL)
write.csv(county_map2, "/Users/paulmj/Documents/01_VECTOR/UH_CIVIC/Harris_census_2018.csv", row.names = TRUE)
year=2019
### Geometry base - US Counties
options(tigris_use_cache = TRUE) #to cache shapefiles for future sessions
state = "Texas"
county = c("Harris County")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
county_map = county_map %>% select(GEOID, NAME)
### Data from ACS-5yr
acs_vars_use = c("B01001B_001", "B01001A_001", "B01003_001")
acs_data = get_acs(geography = "tract", state = state, county = county,
variables=acs_vars_use, year = year, geometry = FALSE)
#long to wide format
acs_data_w = acs_data %>%
select(-moe, -NAME) %>% #remove errors of estimates because we're spreading by variable and don't want duplicates, and don't need name b/c joining by GEOID
spread(key=variable, value = estimate)
df = county_map %>%
left_join(acs_data_w, by = "GEOID") %>% #join ACS_5yr data
mutate(QBLACK = B01001B_001 / B01003_001) %>%
mutate(QWHITE = B01001A_001 / B01003_001) %>%
dplyr::select(QWHITE, QBLACK)
white = df %>%
ggplot(aes(fill = QWHITE)) +
geom_sf(color = NA) +
scale_fill_viridis_c(option = "magma")
black = df %>%
ggplot(aes(fill = QBLACK)) +
geom_sf(color = NA) +
scale_fill_viridis_c(option = "magma")
county_map2 = county_map %>%
st_set_geometry(NULL)
write.csv(county_map2, "/Users/paulmj/Documents/01_VECTOR/UH_CIVIC/Harris_censustract_2019acs.csv", row.names = TRUE)
year=2020
### Geometry base - US Counties
options(tigris_use_cache = TRUE) #to cache shapefiles for future sessions
state = "Texas"
county = c("Harris County")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
?get_acs
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2020, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2019, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2019, geometry = TRUE,
cache_table = TRUE, survey = "acs5")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2019, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2020, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2021, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = get_acs(geography = "tract", state = state, county = county,
variables=c("B01003_001"), year = 2019, geometry = TRUE,
cache_table = TRUE, survey = "acs1")
county_map = county_map %>% select(GEOID, NAME)
install.packages("stars")
library(tidyverse)
library(tidycensus)
library(sf)
library(terra)
library(stars)
load("~/Documents/01_VECTOR.nosync/Sandia/TX_group.Rda")
## Root zone data
TX_gdb =  "/Users/paulmj/Downloads/gSSURGO_TX/gSSURGO_TX.gdb"
TX_Valu1 = sf::st_read(dsn = TX_gdb, layer = "Valu1")
TX_group_val1 = TX_group %>% left_join(TX_Valu1, by = c("MUKEY" = "mukey"))
TX_group_val1_100 = TX_group_val1 %>%
filter(!is.na(rootznemc)) %>%
slice(1:100) %>%
select(MUKEY, rootznemc)
?st_rasterize
TX_rast100 = st_rasterize(TX_group_val1_100)
TX_rast100 = st_rasterize(TX_group_val1_100["rootznemc"])
plot(TX_rast100)
TX_rast = st_rasterize(TX_group_val1["rootznemc"])
plot(TX_rast)
rr = ggplot() +
geom_tile(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c()
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c()
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
#geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
geom_stars(data = TX_rast, aes(fill = rootznemc)) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc), alpha = 0.8) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc), alpha = 0.9) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1) +
theme_bw()
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1) +
theme_classic()
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1) +
theme_minimal()
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1)
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1, na.value = "white")
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1, na.value = "gray")
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1, na.value = "gray") +
theme_light()
rr
rr = ggplot() +
geom_stars(data = TX_rast, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1, na.value = "gray") +
theme_minimal()
rr
st_crs(TX_rast)
st_crs(TX_rast)$units_gdal
TX_rast_1k = st_rasterize(TX_group_val1["rootznemc"], dx = 1000, dy = 1000)
rr = ggplot() +
geom_stars(data = TX_rast_1k, aes(x = x, y = y, fill = rootznemc)) +
scale_fill_viridis_c(direction = -1, na.value = "gray") +
theme_minimal()
rr
install.packages("sqldf")
version
install.packages("sqldf")
install.packages("hurricaneexposure")
install.packages("caret")
install.packages("tidymodels")
install.packages("tune")
install.packages("workflows")
load("~/Documents/01_VECTOR.nosync/Sandia/Data/Sandia3_ml.RData")
options(java.parameters = "-Xmx7g")
library(bartMachine)
set_bart_machine_num_cores(4)
library(tidyverse)
library(tidymodels)
library(tidycensus)
library(sf)
library(lme4)
library(corrplot)
library(viridis)
#library(sqldf)
# GBM
show_model_info("boost_tree")
?dials
?set_engine
?ranger::importance
?ranger::ranger
?randomForest::randomForest
install.packages("xgboost")
?xgboost::xgb.train
gb_model = boost_tree(mode = "regression", trees = 1000,
min_n = tune(), tree_depth = tune(), learn_rate = tune(), loss_reduction = tune()) %>%
set_engine(engine = "xgboost")
gb_work = workflow() %>%
add_recipe(hours_recipe) %>%
add_model(gb_model)
gb_grid = dials::grid_max_entropy(parameters(min_n, tree_depth, learn_rate, loss_reduction), size = 100)
?min_n
gb_grid = dials::grid_max_entropy(parameters(min_n(), tree_depth(), learn_rate(), loss_reduction()), size = 100)
plot(gb_grid)
?tune_grid
gb_tune = gb_work %>%
tune_grid(resamples = df_cv,
grid = gb_grid,
metrics = metric_set(yardstick::rmse(), yardstick::rsq()))
gb_tune = gb_work %>%
tune_grid(resamples = df_cv,
grid = gb_grid,
metrics = metric_set(yardstick::rmse, yardstick::rsq))
?control_grid
gb_tune = gb_work %>%
tune_grid(resamples = df_cv,
grid = gb_grid,
metrics = metric_set(yardstick::rmse, yardstick::rsq),
control = tune::control_grid(verbose = T))
show_best(gb_tune, metric = "rmse")
save.image("~/Documents/01_VECTOR.nosync/Sandia/Data/Sandia3_ml.RData")
gb_tune_results = gb_tune %>% collect_metrics()
gb_best = gb_tune %>% select_best(metric = "rmse")
gb_fit = gb_work %>%
finalize_workflow(gb_best) %>%
last_fit(df_split)
gb_test = gb_fit %>% collect_metrics() #metrics evaluated on test sample (b/c last_fit() function)
gb_predictions = gb_fit %>% collect_predictions() #predictions for test sample (b/c last_fit() function)
save.image("~/Documents/01_VECTOR.nosync/Sandia/Data/Sandia3_ml.RData")
load("~/Documents/01_VECTOR.nosync/Sandia/Data/Sandia4_Final_hrs.RData")
plot_filtering_estimates2(gg)
load("~/Documents/01_VECTOR.nosync/Sandia/Data/Sandia4_Final_hrs.RData")
summary(df_hours)
library(tidyverse)
library(tidymodels)
library(tidycensus)
library(sf)
library(corrplot)
library(viridis)
library(doParallel)
library(terra)
library(stars)
library(raster) #make sure ncdf4 package is installed
library(lubridate)
library(spdep)
library(xgboost)
library(DBI)
library(vip)
library(pdp)
tidymodels_prefer()
setwd("~/Documents/01_VECTOR.nosync/Sandia")
#################################################################################################################
###### DATA #####################################################################################################
#################################################################################################################
## Get SQL power outage data
# https://stackoverflow.com/questions/9802680/importing-files-with-extension-sqlite-into-r
# https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html
# https://www.r-bloggers.com/2018/11/quick-guide-r-sqlite/
mydb = RSQLite::dbConnect(drv = RSQLite::SQLite(),
dbname = "./Data/Outages sqlite/AGM_full_CONUS_outages_merra_noaa_summary_Proc31Jan2022.sqlite")
tables = dbListTables(mydb) #see tables
myquery = dbGetQuery(mydb, 'SELECT * FROM summary')
dbDisconnect(mydb)
county_outages = myquery %>%
mutate(start_dt = as.character(as_datetime(start_date))) %>%
mutate(end_dt = as.character(as_datetime(end_date))) %>%
mutate(start_ym = str_extract(start_dt, "^.{7}")) %>%
mutate(end_ym = str_extract(end_dt, "^.{7}")) %>%
dplyr::select(-c(start_date, end_date)) %>%
relocate(c(start_dt, start_ym, end_dt, end_ym), .after = outage_number)
## Get census map
mycrs = 5070 #chose projected coordinate system: EPSG 5070 NAD83 Conus Albers
year=2019 # year for county boundaries
options(tigris_use_cache = TRUE) #cache shapefiles for future sessions
state_list = c("AL", "AR", "AZ", "CA", "CO", "CT", "DE", "FL", "GA", "IA", "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO", "MS", "MT", "NC", "ND", "NE", "NH", "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VA", "VT", "WA", "WI", "WV", "WY")
county_map = get_acs(geography = "county", state = state_list,
variables=c("B01003_001"), year = year, geometry = TRUE,
cache_table = TRUE)
county_map = county_map %>%
mutate(POPULATION = estimate) %>%
dplyr::select(GEOID, NAME, POPULATION)
county_map_proj = county_map %>%
st_transform(mycrs) # project to Conic Equal Area Albers, EPSG:5070
county_map_area = county_map_proj %>%
mutate(AREA = as.vector(st_area(county_map_proj))) %>% #sq-meters; as.vector removes units suffix
mutate(DENSITY = POPULATION / AREA * 1000^2) %>% #population per sq-km
dplyr::select(-c(POPULATION, AREA, NAME))
## Get environmental data
# NLCD (static)
load(file = "./Data/county_map_nlcd.Rda")
county_map_nlcd = county_map_nlcd %>%
dplyr::select(-c(NAME, POPULATION, ID, Other, Total)) %>%
st_set_geometry(NULL)
# DEM (static)
load(file = "./Data/county_map_dem.Rda")
county_map_dem = county_map_dem %>%
select(GEOID, DEM_mean:DEM_max) %>%
st_set_geometry(NULL)
# Root Zone (static)
load(file = "./Data/county_map_rz.Rda")
county_map_rz = county_map_rz %>%
select(GEOID, RZ_mean:RZ_mode) %>%
st_set_geometry(NULL)
# SPI (monthly)
load(file = "./Data/county_map_spi.Rda")
county_map_spi = county_map_spi %>%
mutate(date_ym = str_extract(Date, "^.{7}")) %>% # returns yyyy-mm
dplyr::select(-NAME, -Date)
## Socio-Economic Data
# ## County-level indicators
load(file = "./Data/final_data.Rda")
county_social = final_data %>%
dplyr::select(-NAME, -POPULATION) %>%
st_set_geometry(NULL)
## Join tables
county_map_JOIN = county_map_area %>%
inner_join(county_outages, by = c("GEOID" = "fips_code")) %>%
left_join(county_map_spi, by = c("GEOID" = "GEOID", "start_ym" = "date_ym")) %>%
left_join(county_map_nlcd, by = c("GEOID")) %>%
left_join(county_map_dem, by = c("GEOID")) %>%
left_join(county_map_rz, by = c("GEOID")) %>%
left_join(county_social, by = c("GEOID"))
county_map_ALL = county_map_JOIN %>%
dplyr::select(-c(outage_number, start_dt, start_ym, end_dt, end_ym)) %>%
dplyr::select(-c(max_WIND2M, max_WIND50M, min_humidity_2M, mean_humidity_2M, max_humidity_2M,
min_T2M, mean_T2M, max_T2M, delta_T2M, min_T10M_C:delta_T10M_C, WETLAND
)) #take extraneous variables
rm(list=setdiff(ls(), c("county_map_JOIN", "county_map_ALL")))
gc()
##########################################################################################################
#### PRE-PROCESSING ######################################################################################
##########################################################################################################
## Clean data-frame
df_data = county_map_ALL %>%
#dplyr::filter(duration_hr > quantile(county_map_ALL$duration_hr, .9)) %>% #filter to big events
dplyr::filter(duration_hr >= 12) %>% #filter to events >12 hrs ... 95% quantile for full dataset
dplyr::filter(duration_hr < 3000) %>% #get rid of faulty data
mutate(ln_cust = log(max_pct_affected), ln_hrs = log(duration_hr)) %>% # take log of DVs
dplyr::select(-c(max_pct_affected, duration_hr, GEOID)) %>%
relocate(c(ln_cust, ln_hrs)) %>%
st_set_geometry(NULL)
num_cores = detectCores() - 1
#function to un-register parallel processing in doParallel
unregister_dopar = function() {
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)
}
## Split into training vs testing
set.seed(23)
df_split = initial_split(df_data, prop = 0.80, strata = "ln_hrs")
df_train = training(df_split)
df_test = testing(df_split)
df_cv = vfold_cv(df_train, v = 10, repeats = 1)
## Pre-processing
cust_recipe = recipe(ln_cust ~ . , data = df_data) %>%
step_rm(ln_hrs) %>%
step_impute_knn(all_predictors()) %>% #knn impute missing predictors (if any)
# step_normalize(all_predictors()) %>% #z-score standardize all predictors (important for PLS or NN)
step_zv(all_predictors()) %>% #removes predictors of single value
step_corr(all_predictors())  #removes highly correlated
df_cust = prep(cust_recipe) %>% juice() #apply recipe to data frame
cust_prep = prep(cust_recipe) #see changes
hours_recipe = recipe(ln_hrs ~ . , data = df_data) %>%
step_rm(ln_cust) %>%
step_impute_knn(all_predictors()) %>% #knn impute missing predictors (if any)
# step_normalize(all_predictors()) %>% #z-score standardize all predictors (important for PLS or NN)
step_zv(all_predictors()) %>% #removes predictors of single value
step_corr(all_predictors())  #removes highly correlated
hours_prep = prep(hours_recipe) #shows changes
df_hours = prep(hours_recipe) %>% juice() #apply recipe to data frame
##########################################################################################################
#### MACHINE LEARNING ####################################################################################
##########################################################################################################
X = df_hours %>% dplyr::select(-ln_hrs)
y = df_hours %>% dplyr::select(ln_hrs) %>% pull()
#https://www.tidyverse.org/blog/2020/11/tune-parallel/
## Lasso/Ridge/ElasticNet
show_model_info("linear_reg")
lre_model = linear_reg(penalty = tune(), mixture = tune()) %>% #lambda (penalty) and alpha/mixture (1 lasso, 0 ridge)
set_engine("glmnet") %>%
translate()
lre_work = workflow() %>%
#add_recipe(cust_recipe) %>%
add_recipe(hours_recipe) %>%
add_model(lre_model)
#lre_grid = dials::grid_regular(parameters(penalty(), mixture()), levels = c(5, 5))
set.seed(32); lre_grid = dials::grid_max_entropy(parameters(penalty(), mixture()), size = 40)
lre_tune = lre_work %>%
tune_grid(resamples = df_cv,
grid = lre_grid,
metrics = metric_set(yardstick::rmse, yardstick::rsq),
control = tune::control_grid(verbose = T)#, allow_par = T, parallel_over = "resamples")
) #parallel processing turns off verbose
cl = makeCluster(num_cores, type = "FORK")
registerDoParallel(cl, cores = num_cores)
lre_tune = lre_work %>%
tune_grid(resamples = df_cv,
grid = lre_grid,
metrics = metric_set(yardstick::rmse, yardstick::rsq),
control = tune::control_grid(verbose = T, allow_par = T, parallel_over = "resamples")
) #parallel processing turns off verbose
stopCluster(cl)
unregister_dopar()
show_best(lre_tune, metric = "rmse")
lre_tune_results = lre_tune %>% collect_metrics()
lre_best = lre_tune %>% select_best(metric = "rmse")
lre_fit = lre_work %>%
finalize_workflow(lre_best) %>%
last_fit(df_split)
lre_test = lre_fit %>% collect_metrics() #metrics evaluated on test sample (b/c last_fit() function)
lre_predictions = lre_fit %>% collect_predictions() #predictions for test sample (b/c last_fit() function)
## Testing results
y_test = df_hours %>% slice(-df_split$in_id) %>% dplyr::select(ln_hrs) %>% pull()
rsq_lre = paste(lre_test %>% dplyr::filter(.metric == "rsq") %>% pull(.estimate) %>% round(3) %>% format(nsmall = 3))
cverror_lre = paste(show_best(lre_tune, metric = "rmse") %>% dplyr::slice(1) %>% pull(mean) %>% round(3) %>% format(nsmall = 3))
show_best(rf_tune, metric = "rmse")
show_best(lre_tune, metric = "rmse")
save.image("~/Documents/01_VECTOR.nosync/Sandia/Data/Sandia4_Final_hrs_2.RData")
cor(df_data$ln_hrs, df_data$total_vapor)
cor(df_data$ln_hrs, df_data$total_liquid)
cor(df_data$total_vapor, df_data$total_liquid)
cor(df_data$ln_hrs, df_data$delta_pressure)
